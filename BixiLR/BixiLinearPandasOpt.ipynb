{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Python worflow we explore the Montreal Bixi biking data set for the year 2017 https://www.kaggle.com/aubertsigouin/biximtl/data\n",
    "\n",
    "We have additionally enriched this data set with the biking distance/duration available via Google map API as gmdata2017\n",
    "\n",
    "Our objective is to predict the \"trip duration\", given the distance between two stations.\n",
    "\n",
    "We are using the Pandas package with an explicitly optimized setting for database connection to transfer data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required pacakges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas.io.sql as psql;\n",
    "import pandas as pd;\n",
    "import pymonetdb.sql;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database connection information. We will try to explicitly optimize the database connection by providing a very high value for the data buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "host='cerberus'; dbname='bixi'; user='bixi'; passwd='bixi'; databuffersize=1000000;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "con = pymonetdb.Connection(dbname,hostname=host,username=user,password=passwd,autocommit=True);\n",
    "con.replysize = databuffersize;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us find out what are the tables available in the database.\n",
    "\n",
    "Python DB API spec does not provide a mechanism to list the tables in the database, so it is left to the users to write a query depending on the RDBMS vendor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      tablename\n",
      "0  stations2017\n",
      "1  tripdata2017\n",
      "2    gmdata2017\n",
      "3          temp\n"
     ]
    }
   ],
   "source": [
    "tblListSQL = \\\n",
    "        \"SELECT t.name as tableName \" \\\n",
    "        \"FROM sys.tables t \" \\\n",
    "        \"  INNER JOIN sys.schemas s \" \\\n",
    "        \"    ON t.schema_id = s.id \" \\\n",
    "        \"WHERE s.name = '{}'\"\\\n",
    "        \";\"\n",
    "\n",
    "tables = psql.read_sql_query(sql=tblListSQL.format('bixi'), con=con);\n",
    "print(tables);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a peek into tripdata2017.\n",
    "For this purpose, we will create a pandas dataframe for tripdata2017 that we can reuse later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id             starttm  stscode               endtm  endscode  duration  \\\n",
      "0   0 2017-04-15 00:00:00     7060 2017-04-15 00:31:00      7060      1841   \n",
      "1   1 2017-04-15 00:01:00     6173 2017-04-15 00:10:00      6173       553   \n",
      "2   2 2017-04-15 00:01:00     6203 2017-04-15 00:04:00      6204       195   \n",
      "3   3 2017-04-15 00:01:00     6104 2017-04-15 00:06:00      6114       285   \n",
      "4   4 2017-04-15 00:01:00     6174 2017-04-15 00:11:00      6174       569   \n",
      "\n",
      "   ismember  \n",
      "0         1  \n",
      "1         1  \n",
      "2         1  \n",
      "3         1  \n",
      "4         1  \n",
      "                 id       stscode      endscode      duration      ismember\n",
      "count  4.018721e+06  4.018721e+06  4.018721e+06  4.018721e+06  4.018721e+06\n",
      "mean   2.009360e+06  6.324815e+03  6.319868e+03  8.374505e+02  7.992535e-01\n",
      "std    1.160105e+06  3.758616e+02  3.832840e+02  6.577148e+02  4.005588e-01\n",
      "min    0.000000e+00  5.002000e+03  5.002000e+03  6.100000e+01  0.000000e+00\n",
      "25%    1.004680e+06  6.105000e+03  6.092000e+03  3.820000e+02  1.000000e+00\n",
      "50%    2.009360e+06  6.203000e+03  6.195000e+03  6.700000e+02  1.000000e+00\n",
      "75%    3.014040e+06  6.389000e+03  6.394000e+03  1.121000e+03  1.000000e+00\n",
      "max    4.018721e+06  1.000200e+04  1.000200e+04  7.199000e+03  1.000000e+00\n"
     ]
    }
   ],
   "source": [
    "tripdata2017 = pd.DataFrame(psql.read_sql_query('SELECT * FROM tripdata2017;', con));\n",
    "print(tripdata2017.head());\n",
    "print(tripdata2017.describe());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 4 million + records in tripdata2017. Also, the station codes are labels. We may have to enrich this information. Let us take a look at the contents of stations2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   scode                      sname  slatitude  slongitude  sispublic\n",
      "0   7060  \"de l'Église / de Verdun\"  45.463001  -73.571569          1\n",
      "1   6173         \"Berri / Cherrier\"  45.519088  -73.569509          1\n",
      "2   6203   \"Hutchison / Sherbrooke\"  45.507810  -73.572080          1\n",
      "3   6204        \"Milton / Durocher\"  45.508144  -73.574772          1\n",
      "4   6104    \"Wolfe / René-Lévesque\"  45.516818  -73.554188          1\n",
      "              scode   slatitude  slongitude   sispublic\n",
      "count    546.000000  546.000000  546.000000  546.000000\n",
      "mean    6412.743590   45.519109  -73.582622    0.983516\n",
      "std      405.396125    0.027919    0.027344    0.127442\n",
      "min     5002.000000   45.430740  -73.670634    0.000000\n",
      "25%     6143.500000   45.501726  -73.599579    1.000000\n",
      "50%     6305.500000   45.523103  -73.577413    1.000000\n",
      "75%     6722.750000   45.538784  -73.564608    1.000000\n",
      "max    10002.000000   45.582757  -73.495067    1.000000\n"
     ]
    }
   ],
   "source": [
    "stations2017 = pd.DataFrame(psql.read_sql_query('SELECT * FROM stations2017;', con));\n",
    "print(stations2017.head());\n",
    "print(stations2017.describe());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is good, we have the longitude and latitude associated with each station, which can be used to enrich the tripdata.\n",
    "\n",
    "Since we have 546 stations, this gives the possibility of 546 x 546 = 298116 possible scenarios for trips. However, we need not be concerned with trips that started and ended at the same station as those are noise. Also, to weed out any further fluctuations in the input data set, we will limit ourselves to only those station combinations which has at the least 50 trips.\n",
    "\n",
    "For this purpose, we will use some relational-API like feature of Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     stscode  endscode  numtrips\n",
      "4     5002.0    5007.0      99.0\n",
      "44    5003.0    5007.0      86.0\n",
      "90    5004.0    5007.0     200.0\n",
      "123   5005.0    5007.0     180.0\n",
      "172   5006.0    5007.0     221.0\n",
      "            stscode      endscode      numtrips\n",
      "count  19300.000000  19300.000000  19300.000000\n",
      "mean    6302.127358   6291.781295    116.905855\n",
      "std      349.880754    351.217448    117.250651\n",
      "min     5002.000000   5002.000000     50.000000\n",
      "25%     6100.000000   6078.000000     62.000000\n",
      "50%     6194.000000   6180.000000     81.000000\n",
      "75%     6350.000000   6362.000000    125.000000\n",
      "max    10002.000000  10002.000000   2200.000000\n"
     ]
    }
   ],
   "source": [
    "freqStations = tripdata2017.where(tripdata2017['stscode'] != tripdata2017['endscode']).dropna();\n",
    "freqStations = pd.DataFrame({'numtrips' : freqStations.groupby(['stscode', 'endscode']).size()}).reset_index();\n",
    "freqStations = freqStations.where(freqStations['numtrips'] >= 50).dropna();\n",
    "print(freqStations.head());\n",
    "print(freqStations.describe());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 19,300 station combinations that is of interest to us. Next we will include the longitude and latitude information of the start and end stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  stscode endscode  numtrips      stlat     stlong      enlat     enlong\n",
      "0    5002     5007      99.0  45.533703 -73.515283  45.523854 -73.519677\n",
      "1    5003     5007      86.0  45.529512 -73.517691  45.523854 -73.519677\n",
      "2    5004     5007     200.0  45.539824 -73.508752  45.523854 -73.519677\n",
      "3    5005     5007     180.0  45.536378 -73.512642  45.523854 -73.519677\n",
      "4    5006     5007     221.0  45.537226 -73.495067  45.523854 -73.519677\n"
     ]
    }
   ],
   "source": [
    "freqStationsCord = pd.merge(freqStations, stations2017, left_on='stscode', right_on='scode') \\\n",
    "                    .loc[:,['stscode', 'endscode', 'numtrips', 'slatitude', 'slongitude']] \\\n",
    "                    .rename(index=str, columns={'slatitude':'stlat', 'slongitude':'stlong'});\n",
    "freqStationsCord = pd.merge(freqStationsCord, stations2017, left_on='endscode', right_on='scode') \\\n",
    "                    .loc[:,['stscode', 'endscode', 'numtrips', 'stlat', 'stlong', 'slatitude', 'slongitude']] \\\n",
    "                    .rename(index=str, columns={'slatitude':'enlat', 'slongitude':'enlong'});\n",
    "\n",
    "print(freqStationsCord.head());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be easier if we can translate the coordinates to a distance metric. Python's geopy module supports this computation using Vincenty's formula. This provides us with a distance as crow flies between two coordiantes. This might be a reasonable approximation of actual distance travelled in a trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  stscode endscode  numtrips      stlat     stlong      enlat     enlong  \\\n",
      "0    5002     5007      99.0  45.533703 -73.515283  45.523854 -73.519677   \n",
      "1    5003     5007      86.0  45.529512 -73.517691  45.523854 -73.519677   \n",
      "2    5004     5007     200.0  45.539824 -73.508752  45.523854 -73.519677   \n",
      "3    5005     5007     180.0  45.536378 -73.512642  45.523854 -73.519677   \n",
      "4    5006     5007     221.0  45.537226 -73.495067  45.523854 -73.519677   \n",
      "\n",
      "   vdistm  \n",
      "0    1147  \n",
      "1     647  \n",
      "2    1969  \n",
      "3    1496  \n",
      "4    2429  \n"
     ]
    }
   ],
   "source": [
    "import geopy.distance;     #We will use this module to compute distance.\n",
    "def computeDist(trip):\n",
    "    #These are the inputs to Vincenty's formula.\n",
    "    stlat = trip['stlat']; stlong = trip['stlong']; enlat = trip['enlat']; enlong = trip['enlong'];\n",
    "    #populate the distance metric using longitude/latitude of coordinates.\n",
    "    return int(geopy.distance.distance((stlat,stlong), (enlat,enlong)).meters);\n",
    "\n",
    "freqStationsDist = pd.DataFrame(freqStationsCord);\n",
    "freqStationsDist['vdistm'] = freqStationsDist.apply(computeDist, axis=1);\n",
    "print(freqStationsDist.head());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can next enrich our trip data set with the distance information by joining these computed distances with each trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  duration  vdistm\n",
      "0      2       195     213\n",
      "1   8213      3152     213\n",
      "2  22734       124     213\n",
      "3  34304        97     213\n",
      "4  54111        92     213\n",
      "                 id      duration        vdistm\n",
      "count  2.256283e+06  2.256283e+06  2.256283e+06\n",
      "mean   2.003455e+06  6.304499e+02  1.369893e+03\n",
      "std    1.162663e+06  5.334432e+02  9.212933e+02\n",
      "min    2.000000e+00  6.100000e+01  4.700000e+01\n",
      "25%    9.945115e+05  2.980000e+02  7.110000e+02\n",
      "50%    2.005024e+06  4.820000e+02  1.117000e+03\n",
      "75%    3.010572e+06  7.930000e+02  1.755000e+03\n",
      "max    4.018721e+06  7.199000e+03  9.074000e+03\n"
     ]
    }
   ],
   "source": [
    "tripData = pd.merge(tripdata2017, freqStationsDist, on=['stscode', 'endscode']) \\\n",
    "             .loc[:,['id', 'duration', 'vdistm']];\n",
    "print(tripData.head());\n",
    "print(tripData.describe());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have trip duration for each trip and the distance as crow flies, between the two stations involved in the trip.\n",
    "\n",
    "Also, we have about 2 million trips for which we have distance between stations metric. Given that there are only a few thousand unique values for distance, we might want to keep some values of distance apart for testing. For this purpose, we will first get distinct values for distance and then sort it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         vdistm\n",
      "2244879      47\n",
      "56654        71\n",
      "1898286      81\n",
      "1776285      85\n",
      "1011539      88\n",
      "         vdistm\n",
      "908190     8529\n",
      "1841645    8752\n",
      "1971339    8860\n",
      "1761199    9031\n",
      "2197167    9074\n",
      "            vdistm\n",
      "count  3652.000000\n",
      "mean   2203.772453\n",
      "std    1386.237638\n",
      "min      47.000000\n",
      "25%    1095.750000\n",
      "50%    2013.500000\n",
      "75%    3068.250000\n",
      "max    9074.000000\n"
     ]
    }
   ],
   "source": [
    "uniqueTripDist = tripData.loc[:,['vdistm']].drop_duplicates().sort_values(by=['vdistm']);\n",
    "\n",
    "print(uniqueTripDist.head());\n",
    "print(uniqueTripDist.tail());\n",
    "print(uniqueTripDist.describe());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will keep some data apart for testing. A rule of thumb is 30%. The neat trick below sets apart 33%, across the entire range of distance values. close enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         vdistm\n",
      "2244879      47\n",
      "1776285      85\n",
      "545957      110\n",
      "1783        126\n",
      "1462413     148\n",
      "         vdistm\n",
      "2238694    6796\n",
      "1074925    7057\n",
      "2192277    7530\n",
      "1841645    8752\n",
      "2197167    9074\n",
      "            vdistm\n",
      "count  1218.000000\n",
      "mean   2205.059113\n",
      "std    1391.487164\n",
      "min      47.000000\n",
      "25%    1095.750000\n",
      "50%    2013.500000\n",
      "75%    3068.250000\n",
      "max    9074.000000\n"
     ]
    }
   ],
   "source": [
    "testTripDist = uniqueTripDist[::3];\n",
    "print(testTripDist.head());\n",
    "print(testTripDist.tail());\n",
    "print(testTripDist.describe());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us get the remaining values for distances to be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         vdistm\n",
      "56654        71\n",
      "1898286      81\n",
      "1011539      88\n",
      "775871       94\n",
      "1299438     120\n",
      "         vdistm\n",
      "812754     7307\n",
      "2074486    7650\n",
      "908190     8529\n",
      "1971339    8860\n",
      "1761199    9031\n",
      "            vdistm\n",
      "count  2434.000000\n",
      "mean   2203.128595\n",
      "std    1383.889269\n",
      "min      71.000000\n",
      "25%    1096.250000\n",
      "50%    2013.500000\n",
      "75%    3067.750000\n",
      "max    9031.000000\n"
     ]
    }
   ],
   "source": [
    "trainTripDist = uniqueTripDist[~uniqueTripDist['vdistm'].isin(testTripDist['vdistm'])];\n",
    "print(trainTripDist.head());\n",
    "print(trainTripDist.tail());\n",
    "print(trainTripDist.describe());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now extract the fields of interest to us for the training data, which is just the distance of each trip and it's duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   vdistm  duration\n",
      "0     213       195\n",
      "1     213      3152\n",
      "2     213       124\n",
      "3     213        97\n",
      "4     213        92\n",
      "         vdistm  duration\n",
      "2256278    1433       472\n",
      "2256279    1433       511\n",
      "2256280    1433       774\n",
      "2256281    1433       517\n",
      "2256282    1433       579\n",
      "             vdistm      duration\n",
      "count  1.503408e+06  1.503408e+06\n",
      "mean   1.370759e+03  6.294150e+02\n",
      "std    9.262827e+02  5.360811e+02\n",
      "min    7.100000e+01  6.100000e+01\n",
      "25%    7.060000e+02  2.950000e+02\n",
      "50%    1.109000e+03  4.800000e+02\n",
      "75%    1.774000e+03  7.930000e+02\n",
      "max    9.031000e+03  7.199000e+03\n"
     ]
    }
   ],
   "source": [
    "trainData = tripData[tripData['vdistm'].isin(trainTripDist['vdistm'])];\n",
    "trainData = trainData.loc[:, ['vdistm', 'duration']];\n",
    "print(trainData.head());\n",
    "print(trainData.tail());\n",
    "print(trainData.describe());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the values are huge, we should normalize the data attributes. First get the max values for these attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9074\n",
      "7199\n"
     ]
    }
   ],
   "source": [
    "maxdist = uniqueTripDist['vdistm'].max();\n",
    "print(maxdist);\n",
    "maxduration = tripData['duration'].max();\n",
    "print(maxduration);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us normalize the training data. As we are working with integer data, we will also have to convert it to float. That can be accomplished by multiplying with 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     vdistm  duration\n",
      "0  0.023474  0.027087\n",
      "1  0.023474  0.437839\n",
      "2  0.023474  0.017225\n",
      "3  0.023474  0.013474\n",
      "4  0.023474  0.012780\n",
      "           vdistm  duration\n",
      "2256278  0.157924  0.065565\n",
      "2256279  0.157924  0.070982\n",
      "2256280  0.157924  0.107515\n",
      "2256281  0.157924  0.071816\n",
      "2256282  0.157924  0.080428\n"
     ]
    }
   ],
   "source": [
    "trainData['vdistm'] = trainData['vdistm']/maxdist;\n",
    "trainData['duration'] = trainData['duration']/maxduration;\n",
    "\n",
    "print(trainData.head());\n",
    "print(trainData.tail());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our linear regression equation is of the form.\n",
    "\n",
    "dur = a + b*dist\n",
    "\n",
    "we will re-organize the training data set to fit this format and also setup our initial parameters for a and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   x0    vdistm\n",
      "0   1  0.023474\n",
      "1   1  0.023474\n",
      "2   1  0.023474\n",
      "3   1  0.023474\n",
      "4   1  0.023474\n",
      "   duration\n",
      "0  0.027087\n",
      "1  0.437839\n",
      "2  0.017225\n",
      "3  0.013474\n",
      "4  0.012780\n",
      "          vdistm  x0\n",
      "duration       1   1\n"
     ]
    }
   ],
   "source": [
    "trainDataSet = trainData.loc[:, ['vdistm']];\n",
    "trainDataSet.insert(0, 'x0', 1);\n",
    "print(trainDataSet.head());\n",
    "trainDataSetDuration = trainData.loc[:, ['duration']];\n",
    "print(trainDataSetDuration.head());\n",
    "#With pandas data frames, to do matrix multiplication, the column names should match.\n",
    "params = pd.DataFrame({'x0':[1], 'vdistm':[1]}, index=['duration']);\n",
    "print(params);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to run a prediction using these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   duration\n",
      "0  1.023474\n",
      "1  1.023474\n",
      "2  1.023474\n",
      "3  1.023474\n",
      "4  1.023474\n"
     ]
    }
   ],
   "source": [
    "pred = trainDataSet.dot(params.T);\n",
    "print(pred.head());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to compute the squared error for the predictions. Since we will be reusing them, we might as well store it as a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def squaredErr(actual, predicted):\n",
    "    return ((predicted-actual)**2).sum()/(2*(actual.shape[0]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see what is the error for the first iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration    0.569487\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sqerr = squaredErr(trainDataSetDuration, pred);\n",
    "print(sqerr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to perform a gradient descent based on the squared errors. We will write another function to perform this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradDesc(actual, predicted, indata):\n",
    "    return (predicted-actual).T.dot(indata) / actual.shape[0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us update our params using gradient descent using the error we got. We also need to use a learning rate, alpha (arbitrarily chosen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            vdistm        x0\n",
      "duration  0.983306  0.893637\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1;\n",
    "\n",
    "params = params - alpha * gradDesc(trainDataSetDuration, pred, trainDataSet);\n",
    "print(params);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try to use the updated params to train the model again and see if the error is decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   duration\n",
      "0  0.916718\n",
      "1  0.916718\n",
      "2  0.916718\n",
      "3  0.916718\n",
      "4  0.916718\n",
      "duration    0.459497\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "pred = trainDataSet.dot(params.T);\n",
    "print(pred.head());\n",
    "sqerr = squaredErr(trainDataSetDuration, pred);\n",
    "print(sqerr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed, may be we should check if google maps API's distance metric gives a better learning rate. Let us see what fields we can use from Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   stscode  endscode  gdistm  gduration\n",
      "0     6406      6052    3568        596\n",
      "1     6050      6406    3821        704\n",
      "2     6148      6173    1078        293\n",
      "3     6110      6114    1319        337\n",
      "4     6123      6114     725        177\n",
      "            stscode      endscode        gdistm     gduration\n",
      "count  19516.000000  19516.000000  19516.000000  19516.000000\n",
      "mean    6302.622361   6291.736729   2079.293093    516.190869\n",
      "std      350.305819    350.469248   1345.629279    299.191977\n",
      "min     5002.000000   5002.000000     18.000000      4.000000\n",
      "25%     6100.000000   6078.000000   1118.000000    300.000000\n",
      "50%     6194.000000   6180.000000   1766.000000    459.000000\n",
      "75%     6350.000000   6362.000000   2711.000000    671.000000\n",
      "max    10002.000000  10002.000000  14530.000000   3083.000000\n"
     ]
    }
   ],
   "source": [
    "gmdata2017 = pd.DataFrame(psql.read_sql_query('SELECT * FROM gmdata2017;', con));\n",
    "print(gmdata2017.head());\n",
    "print(gmdata2017.describe());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build a new data set for the trips between frequently used station combination that includes google's distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id  duration  gdistm  gduration\n",
      "0  4152      1561    3568        596\n",
      "1  4154      1482    3568        596\n",
      "2  4985       965    3568        596\n",
      "3  4986       968    3568        596\n",
      "4  6354      1376    3568        596\n",
      "                 id      duration        gdistm     gduration\n",
      "count  2.256283e+06  2.256283e+06  2.256283e+06  2.256283e+06\n",
      "mean   2.003455e+06  6.304499e+02  1.834732e+03  4.546805e+02\n",
      "std    1.162663e+06  5.334432e+02  1.236259e+03  2.733622e+02\n",
      "min    2.000000e+00  6.100000e+01  4.800000e+01  1.100000e+01\n",
      "25%    9.945115e+05  2.980000e+02  9.600000e+02  2.570000e+02\n",
      "50%    2.005024e+06  4.820000e+02  1.497000e+03  3.940000e+02\n",
      "75%    3.010572e+06  7.930000e+02  2.359000e+03  5.910000e+02\n",
      "max    4.018721e+06  7.199000e+03  1.453000e+04  3.083000e+03\n"
     ]
    }
   ],
   "source": [
    "gtripData = pd.merge(gmdata2017, tripdata2017, on=['stscode', 'endscode']) \\\n",
    "              .loc[:,['stscode', 'endscode', 'id', 'duration', 'gdistm', 'gduration']];\n",
    "gtripData = pd.merge(gtripData, freqStations, on=['stscode', 'endscode']) \\\n",
    "              .loc[:,['id', 'duration', 'gdistm', 'gduration']];\n",
    "print(gtripData.head());\n",
    "print(gtripData.describe());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google also provides its estimated duration for the trip. We will have to see in the end if our trained model is able to predict the trip duration better than google's estimate. So we will also save Google's estimate for the trip duration for that comparison.\n",
    "\n",
    "Next up, we need to format this dataset the same way we did the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14530\n",
      "7199\n"
     ]
    }
   ],
   "source": [
    "guniqueTripDist = gtripData.loc[:,['gdistm']].drop_duplicates().sort_values(by=['gdistm']);\n",
    "gtestTripDist = guniqueTripDist[::3];\n",
    "\n",
    "gtrainTripDist = guniqueTripDist[~guniqueTripDist['gdistm'].isin(gtestTripDist['gdistm'])];\n",
    "gtrainData = gtripData[gtripData['gdistm'].isin(gtrainTripDist['gdistm'])];\n",
    "gtrainData = gtrainData.loc[:, ['gdistm', 'duration']];\n",
    "\n",
    "gmaxdist = guniqueTripDist['gdistm'].max();\n",
    "print(gmaxdist);\n",
    "gmaxduration = gtripData['duration'].max();\n",
    "print(gmaxduration);\n",
    "gtrainData['gdistm'] = gtrainData['gdistm']/gmaxdist;\n",
    "gtrainData['duration'] = gtrainData['duration']/gmaxduration;\n",
    "\n",
    "gtrainDataSet = gtrainData.loc[:, ['gdistm']];\n",
    "gtrainDataSet.insert(0, 'x0', 1);\n",
    "gtrainDataSetDuration = gtrainData.loc[:, ['duration']];\n",
    "gparams = pd.DataFrame({'x0':[1], 'gdistm':[1]}, index=['duration']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how the error rate is progressing for the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration    0.541968\n",
      "dtype: float64\n",
      "duration    0.437908\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "gpred = gtrainDataSet.dot(gparams.T);\n",
    "gsqerr = squaredErr(gtrainDataSetDuration, gpred);\n",
    "print(gsqerr);\n",
    "gparams = gparams - alpha * gradDesc(gtrainDataSetDuration, gpred, gtrainDataSet);\n",
    "gpred = gtrainDataSet.dot(gparams.T);\n",
    "gsqerr = squaredErr(gtrainDataSetDuration, gpred);\n",
    "print(gsqerr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like using Google maps' distance is giving us a slight advantage. That makes sense, since Vincenty's formula computes distances as a crow flies, where as Google maps' distance metric is based on the actual road network distances. Better data gives better prediction results !\n",
    "\n",
    "We are done with the feature selection and feature engineering phase for now.\n",
    "\n",
    "Next we will proceed to train our linear regression model using the training data set.\n",
    "\n",
    "Meanwhile, we will also let it printout the error rate at frequent intervals so that we know it is decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate after 100 iterations is duration    0.002415\n",
      "dtype: float64\n",
      "Error rate after 200 iterations is duration    0.002352\n",
      "dtype: float64\n",
      "Error rate after 300 iterations is duration    0.002297\n",
      "dtype: float64\n",
      "Error rate after 400 iterations is duration    0.00225\n",
      "dtype: float64\n",
      "Error rate after 500 iterations is duration    0.002209\n",
      "dtype: float64\n",
      "Error rate after 600 iterations is duration    0.002173\n",
      "dtype: float64\n",
      "Error rate after 700 iterations is duration    0.002142\n",
      "dtype: float64\n",
      "Error rate after 800 iterations is duration    0.002115\n",
      "dtype: float64\n",
      "Error rate after 900 iterations is duration    0.002092\n",
      "dtype: float64\n",
      "Error rate after 1000 iterations is duration    0.002072\n",
      "dtype: float64\n",
      "            gdistm        x0\n",
      "duration  0.671629  0.002959\n",
      "duration    0.002072\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 1000):\n",
    "    gpred = gtrainDataSet.dot(gparams.T);\n",
    "    gparams = gparams - alpha*gradDesc(gtrainDataSetDuration, gpred, gtrainDataSet);\n",
    "    if((i+1)%100 == 0):\n",
    "        print(\"Error rate after {} iterations is {}\".format(i+1, squaredErr(gtrainDataSetDuration, gpred)))\n",
    "    \n",
    "print(gparams);\n",
    "gsqerr = squaredErr(gtrainDataSetDuration, gpred);\n",
    "print(gsqerr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how our model performs in predictions against the test data set we had kept apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration    99215.984246\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "gtestData = gtripData[gtripData['gdistm'].isin(gtestTripDist['gdistm'])];\n",
    "gtestData = gtestData.loc[:, ['gdistm', 'duration', 'gduration']];\n",
    "gtestData['gdistm'] = gtestData['gdistm']/gmaxdist;\n",
    "gtestData['duration'] = gtestData['duration']/gmaxduration;\n",
    "gtestDataSet = gtestData.loc[:, ['gdistm']];\n",
    "gtestDataSet.insert(0, 'x0', 1);\n",
    "gtestDataSetDuration = gtestData.loc[:, ['duration']];\n",
    "\n",
    "gtestpred = gtestDataSet.dot(gparams.T);\n",
    "\n",
    "gtestsqerr1 = squaredErr(gtestDataSetDuration*gmaxduration, gtestpred*gmaxduration);\n",
    "print(gtestsqerr1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would also like to check how the duration provided by Google maps' API hold up to the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration    111763.379836\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "gtestsqerr2 = squaredErr(gtestDataSetDuration*gmaxduration, gtestData.loc[:,['gduration']] \\\n",
    "                         .rename(columns={'gduration':'duration'}));\n",
    "print(gtestsqerr2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So yes, our model is able to do a better job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
